{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['<s>', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '<eos>']\n",
      "Vocabulary: {'A': 4, 'I': 11, 'Y': 23, 'R': 18, 'V': 21, '<mask>': 1, '<eos>': 2, 'E': 7, 'G': 9, 'M': 14, 'Q': 17, 'C': 5, 'F': 8, 'L': 13, 'P': 16, 'D': 6, 'N': 15, 'W': 22, 'H': 10, 'S': 19, 'T': 20, 'K': 12, '<pad>': 0, '<s>': 3}\n",
      "Vocabulary size: 24\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "# Define the alphabet for protein sequences\n",
    "protein_alphabet = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "# Create a tokenizer with a BPE model\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Define a pre-tokenizer that splits on each character\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# Define a decoder\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Define a trainer with the protein alphabet\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=len(protein_alphabet) + 3,  # +3 for <pad>, <eos>, and <s>\n",
    "    special_tokens=[\"<pad>\", \"<mask>\", \"<eos>\", \"<s>\"],\n",
    "    initial_alphabet=list(protein_alphabet)\n",
    ")\n",
    "\n",
    "# Train the tokenizer on a list of protein sequences\n",
    "protein_sequences = [\"ACDEFGHIKLMNPQRSTVWY\", \"ACDEFGHIKLMNPQRSTVWY\", \"ACDEFGHIKLMNPQRSTVWY\"]\n",
    "tokenizer.train_from_iterator(protein_sequences, trainer=trainer)\n",
    "\n",
    "# Add post-processing to handle special tokens\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<s> $A <eos>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"protein_tokenizer.json\")\n",
    "\n",
    "# Example usage\n",
    "encoded = tokenizer.encode(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "print(encoded.tokens)\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 208\n",
      "Validation dataset size: 26\n",
      "Test dataset size: 27\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "df = pd.read_csv(\"../ML4Proteins/01_Introduction/snake_venoms/Snake_Toxins_with_Function_Classes.csv\")\n",
    "# Delete entries where df['Sequence'] is longer than 600\n",
    "df = df[df['Sequence'].str.len() <= seq_length-1]\n",
    "\n",
    "# Tokenize the sequences\n",
    "df['Tokenized Sequence'] = df['Sequence'].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert tokenized sequences to numerical IDs\n",
    "def tokenize_sequence(sequence):\n",
    "    return [tokenizer.token_to_id(token) for token in sequence]\n",
    "\n",
    "train_sequences = train_df['Tokenized Sequence'].apply(tokenize_sequence).tolist()\n",
    "val_sequences = val_df['Tokenized Sequence'].apply(tokenize_sequence).tolist()\n",
    "test_sequences = test_df['Tokenized Sequence'].apply(tokenize_sequence).tolist()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_tensors = [torch.tensor(seq, dtype=torch.long) for seq in train_sequences]\n",
    "val_tensors = [torch.tensor(seq, dtype=torch.long) for seq in val_sequences]\n",
    "test_tensors = [torch.tensor(seq, dtype=torch.long) for seq in test_sequences]\n",
    "\n",
    "# Apply padding to the sequences till length 600\n",
    "train_tensors = torch.nn.utils.rnn.pad_sequence(train_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "val_tensors = torch.nn.utils.rnn.pad_sequence(val_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "test_tensors = torch.nn.utils.rnn.pad_sequence(test_tensors, batch_first=True, padding_value=tokenizer.token_to_id(\"<pad>\"))\n",
    "\n",
    "# Ensure all sequences are of length 600\n",
    "train_tensors = torch.nn.functional.pad(train_tensors, (0, seq_length - train_tensors.size(1)), value=tokenizer.token_to_id(\"<pad>\"))\n",
    "val_tensors = torch.nn.functional.pad(val_tensors, (0, seq_length - val_tensors.size(1)), value=tokenizer.token_to_id(\"<pad>\"))\n",
    "test_tensors = torch.nn.functional.pad(test_tensors, (0, seq_length - test_tensors.size(1)), value=tokenizer.token_to_id(\"<pad>\"))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_tensors)\n",
    "val_dataset = TensorDataset(val_tensors)\n",
    "test_dataset = TensorDataset(test_tensors)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/85/8/203063/pai_course/pai/lib64/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "7it [00:10,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.1164\n",
      "Validation Loss: 3.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:10,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 3.0117\n",
      "Validation Loss: 3.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:07,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 3.0028\n",
      "Validation Loss: 3.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:07,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 2.9813\n",
      "Validation Loss: 3.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 2.9727\n",
      "Validation Loss: 3.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:07,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 2.9478\n",
      "Validation Loss: 3.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:07,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 2.9392\n",
      "Validation Loss: 3.0293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:09,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 2.9280\n",
      "Validation Loss: 3.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 105\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# targets = torch.full((logits.shape[0], logits.shape[1], logits.shape[2]), fill_value=0, device=device, dtype=torch.float, requires_grad=False)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# for batcn_ix in range(logits.shape[0]):\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#     for seq_ix in range(logits.shape[1]):\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#         targets[batcn_ix, seq_ix, inputs[batcn_ix, seq_ix]] = 1\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#print(outputs.shape)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 105\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/pai_course/pai/lib64/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pai_course/pai/lib64/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pai_course/pai/lib64/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        \n",
    "        # Latent layer\n",
    "        self.latent_layer = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.relatent_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "        \n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_embedded = self.embedding(x).transpose(0, 1)\n",
    "        memory = self.encoder(x_embedded)\n",
    "        memory = self.latent_layer(memory.mean(dim=0))\n",
    "        return memory.unsqueeze(0)\n",
    "\n",
    "    def decode(self, memory, max_length, start_token, eos_token):\n",
    "        memory = self.relatent_layer(memory)\n",
    "        batch_size = memory.size(1)\n",
    "        device = memory.device\n",
    "        decoded_tokens = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)\n",
    "        logit_list = torch.full((batch_size, self.max_seq_length, vocab_size), -float('inf'), dtype=torch.float16, device=device)\n",
    "        #logit_list[:, :, 0] = \n",
    "        #print(logit_list.shape)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            tgt_emb = self.embedding(decoded_tokens).transpose(0, 1)\n",
    "            out = self.decoder(tgt_emb, memory)\n",
    "            logits = self.output_layer(out[-1])\n",
    "            logit_list[:, t, :] = logits\n",
    "            next_token = torch.argmax(logits, dim=1, keepdim=True)\n",
    "            decoded_tokens = torch.cat([decoded_tokens, next_token], dim=1)\n",
    "\n",
    "            if torch.all(next_token.eq(eos_token)):\n",
    "                break\n",
    "\n",
    "        return decoded_tokens, logit_list\n",
    "\n",
    "    def forward(self, x, max_length=None, start_token=3, eos_token=2):\n",
    "        if max_length is None:\n",
    "            max_length = self.max_seq_length\n",
    "\n",
    "        memory = self.encode(x)\n",
    "        decoded_tokens, logits = self.decode(memory, max_length, start_token, eos_token)\n",
    "        return decoded_tokens, logits\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = 24\n",
    "embedding_dim = 128\n",
    "latent_dim = 32\n",
    "max_seq_length = 50  # Set a default value for max_seq_length\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(vocab_size, embedding_dim, latent_dim, max_seq_length).to(device)\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "accumulation_steps = 4  # Number of steps to accumulate gradients\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs, logits = model(inputs)\n",
    "        \n",
    "\n",
    "\n",
    "        # targets = torch.full((logits.shape[0], logits.shape[1], logits.shape[2]), fill_value=0, device=device, dtype=torch.float, requires_grad=False)\n",
    "        # for batcn_ix in range(logits.shape[0]):\n",
    "        #     for seq_ix in range(logits.shape[1]):\n",
    "        #         targets[batcn_ix, seq_ix, inputs[batcn_ix, seq_ix]] = 1\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(logits.view(-1, vocab_size), inputs.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            outputs, logits = model(inputs)\n",
    "            \n",
    "            # Reshape outputs and targets for the loss function\n",
    "\n",
    "            # targets = torch.full((logits.shape[0], logits.shape[1], logits.shape[2]), fill_value=0, device=device, dtype=torch.float, requires_grad=False)\n",
    "            # for batcn_ix in range(logits.shape[0]):\n",
    "            #     for seq_ix in range(logits.shape[1]):\n",
    "            #         targets[batcn_ix, seq_ix, inputs[batcn_ix, seq_ix]] = 1\n",
    "\n",
    "            loss = criterion(logits.view(-1,vocab_size), inputs.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 51])\n",
      "['<mask>', 'C', 'E', None, 'F', 'N', None, '<mask>', 'A', 'S', 'H', None, 'G', 'L', 'E', None, '<s>', '<mask>', 'A', None, 'D', '<mask>', 'A', None, 'L', None]\n",
      "['<s>', 'C', 'T', 'T', 'G', 'P', 'C', 'C', 'R', 'Q', 'C', 'K', 'L', 'K', 'P', 'A', 'G', 'T', 'T', 'C', 'W', 'K', 'T', 'S', 'R', 'T', 'S', 'H', 'Y', 'C', 'T', 'G', 'K', 'S', 'C', 'D', 'C', 'P', 'V', 'Y', 'Q', 'G', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'L', 'V', 'S', 'V', 'S', 'P', 'A', 'F', 'N', 'G', 'N', 'Y', 'F', 'V', 'E', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'N', 'L', 'L', 'Q', 'F', 'A', 'F', 'M', 'I', 'R', 'Q', 'A', 'N', 'K', 'R', 'R', 'R', 'P', 'V', 'I', 'P', 'Y', 'E', 'E', 'Y', 'G', 'L', 'Y', 'Y', 'M', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'N', 'F', 'S', 'C', 'P', 'P', 'D', 'W', 'Y', 'A', 'Y', 'D', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'T', 'P', 'E', 'H', 'Q', 'R', 'Y', 'V', 'E', 'L', 'F', 'I', 'V', 'V', 'D', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'V', 'P', 'G', 'G', 'D', 'E', 'C', 'N', 'I', 'N', 'E', 'H', 'R', 'S', 'L', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'T', 'L', 'T', 'S', 'F', 'G', 'E', 'W', 'R', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'N', 'V', 'Y', 'Q', 'Y', 'R', 'K', 'M', 'L', 'Q', 'C', 'A', 'M', 'P', 'N', 'G', 'G', 'P', 'F', 'E', 'C', 'C', 'Q', 'T', 'H', 'D', 'N', 'C', 'Y', 'G', 'E', 'A', 'E', 'K', 'L', 'K', 'A', 'C', 'T', 'S', 'T', 'H', 'S', 'S', 'P', 'Y', 'F', 'K', '<eos>', '<s>', 'N', 'L', 'V', 'Q', 'F', 'G', 'K', 'M', 'I', 'E', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'E', 'V', 'R', 'P', 'F', 'P', 'E', 'V', 'Y', 'E', 'R', 'I', 'A', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'L', 'T', 'C', 'Y', 'K', 'G', 'Y', 'R', 'D', 'T', 'V', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'D', 'F', 'D', 'C', 'P', 'D', 'W', 'V', 'Y', 'D', 'Q', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'K', 'D', 'F', 'C', 'H', 'L', 'P', 'P', 'K', 'P', 'G', 'P', 'C', 'R', 'A', 'A', 'I', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'V', 'I', 'G', 'G', 'D', 'I', 'C', 'N', 'I', 'N', 'E', 'H', 'N', 'F', 'L', 'V', 'A', 'L', 'Y', 'E', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'V', 'V', 'G', 'G', 'D', 'E', 'C', 'N', 'I', 'N', 'E', 'H', 'R', 'S', 'L', 'V', 'A', 'I', 'F', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'H', 'Q', 'K', 'Y', 'N', 'P', 'F', 'R', 'F', 'V', 'E', 'L', 'V', 'L', 'V', 'V', 'D', 'K', 'A', 'M', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'S', 'L', 'G', 'G', 'K', 'P', 'D', 'L', 'R', 'P', 'C', 'H', 'P', 'P', 'C', 'H', 'Y', 'I', 'P', 'R', 'P', 'K', 'P', 'R', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'V', 'I', 'G', 'G', 'D', 'E', 'C', 'D', 'I', 'N', 'E', 'H', 'R', 'F', 'L', 'V', 'F', 'L', 'T', 'A', 'S', 'G', 'L', 'A', 'C', 'G', 'G', 'T', 'L', 'I', 'N', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'S', 'D', 'S', 'K', 'I', 'G', 'D', 'G', 'C', 'F', 'G', 'L', 'P', 'L', 'D', 'H', 'I', 'G', 'S', 'V', 'S', 'G', 'L', 'G', 'C', 'N', 'R', 'P', 'V', 'Q', 'N', 'R', 'P', 'K', 'K', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'D', 'L', 'M', 'Q', 'F', 'E', 'T', 'L', 'I', 'M', 'K', 'I', 'A', 'G', 'R', 'S', 'G', 'V', 'W', 'I', 'Y', 'G', 'S', 'Y', 'G', 'C', 'Y', 'C', 'G', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'R', 'I', 'C', 'L', 'N', 'Q', 'Q', 'S', 'S', 'E', 'P', 'Q', 'T', 'T', 'E', 'I', 'C', 'P', 'D', 'G', 'E', 'D', 'T', 'C', 'Y', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'Y', 'K', 'Q', 'C', 'H', 'K', 'K', 'G', 'G', 'H', 'C', 'F', 'P', 'K', 'E', 'K', 'I', 'C', 'I', 'P', 'P', 'S', 'S', 'D', 'L', 'G', 'K', 'M', 'D', 'C', 'R', 'W', 'K', 'W', 'K', 'C', 'C', 'K', 'K', 'G', 'S', 'G', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'V', 'V', 'G', 'G', 'D', 'E', 'C', 'N', 'I', 'N', 'E', 'H', 'R', 'S', 'L', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'C', 'T', 'C', 'K', 'D', 'M', 'T', 'D', 'K', 'E', 'C', 'L', 'Y', 'F', 'C', 'H', 'Q', 'D', 'I', 'I', 'W', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'L', 'I', 'C', 'Y', 'N', 'D', 'H', 'G', 'Y', 'T', 'G', 'K', 'T', 'T', 'E', 'T', 'C', 'E', 'N', 'G', 'E', 'T', 'T', 'C', 'Y', 'E', 'K', 'S', 'R', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<s>', 'L', 'F', 'D', 'P', 'P', 'D', 'S', 'P', 'Y', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        targets = inputs.clone()\n",
    "\n",
    "        outputs, logits = model(inputs)\n",
    "        print(outputs.shape)\n",
    "        # Reshape outputs and targets for the loss function\n",
    "        outputs = outputs\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Convert outputs to tokens\n",
    "        predicted_tokens = [tokenizer.id_to_token(id.item()) for id in outputs.argmax(dim=1)]\n",
    "        print(predicted_tokens)\n",
    "\n",
    "        # Convert targets to tokens\n",
    "        target_tokens = [tokenizer.id_to_token(id.item()) for id in targets]\n",
    "        print(target_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
