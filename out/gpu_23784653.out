Fri Jan 17 16:03:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:65:00.0 Off |                   On |
| N/A   34C    P0             64W /  300W |      88MiB /  81920MiB |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| MIG devices:                                                                            |
+------------------+----------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|        Shared         |
|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |
|                  |                                  |        ECC|                       |
|==================+==================================+===========+=======================|
|  0    5   0   0  |              25MiB / 19968MiB    | 28      0 |  2   0    1    0    0 |
|                  |                 0MiB / 32767MiB  |           |                       |
+------------------+----------------------------------+-----------+-----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+



['<s>', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '<eos>']
Vocabulary: {'H': 10, 'V': 21, '<s>': 3, 'A': 4, '<mask>': 1, 'G': 9, '<eos>': 2, 'P': 16, 'Y': 23, 'C': 5, 'K': 12, 'R': 18, 'T': 20, 'M': 14, 'N': 15, 'D': 6, 'F': 8, '<pad>': 0, 'I': 11, 'Q': 17, 'L': 13, 'S': 19, 'W': 22, 'E': 7}
Vocabulary size: 24
                               ID                                           Sequence
0  tr|A0A011QCF7|A0A011QCF7_9PROT  MLPGRTPAAQLKLTILHIDARENNMDQSNRYADLSLREEDLIAGGK...
1  tr|A0A021X0E6|A0A021X0E6_9HYPH  MIRLTYRIETAGSPEAMAAKIASDQSTGTFVALPGETEELKARVAA...
2  tr|A0A023CSQ7|A0A023CSQ7_9BACI  MSQVIATYLIHDEKDIKKKAEGIALGLTVGTWTDLPLLEQEQLRKH...
3  tr|A0A023D5D1|A0A023D5D1_ACIMT  MNEITEIRGRDRYRAGVLKYAQMGYWDSDYTPSDTDLLALFRITPQ...
4  tr|A0A023PKS2|A0A023PKS2_9STRA  MFQSVEERTRIKNERYESGVIPYAEMGYWDANYTIKDTDVLALFRI...
Train dataset size: 3080
Validation dataset size: 385
Test dataset size: 385
Batch 0 loss: 2.9000
Batch 1 loss: 2.8860
Batch 2 loss: 2.8764
Batch 3 loss: 2.8723
Batch 4 loss: 2.8665
Batch 5 loss: 2.8653
Batch 6 loss: 2.8604
Batch 7 loss: 2.8559
Batch 8 loss: 2.8530
Batch 9 loss: 2.8507
Batch 10 loss: 2.8480
Batch 11 loss: 2.8459
Batch 12 loss: 2.8433
Batch 13 loss: 2.8417
Batch 14 loss: 2.8400
Batch 15 loss: 2.8376
Batch 16 loss: 2.8366
Batch 17 loss: 2.8351
Batch 18 loss: 2.8331
Batch 19 loss: 2.8330
Batch 20 loss: 2.8321
Batch 21 loss: 2.8310
Batch 22 loss: 2.8296
Batch 23 loss: 2.8282
Batch 24 loss: 2.8274
Batch 25 loss: 2.8267
Batch 26 loss: 2.8261
Batch 27 loss: 2.8249
Batch 28 loss: 2.8241
Batch 29 loss: 2.8237
Batch 30 loss: 2.8239
Batch 31 loss: 2.8234
Batch 32 loss: 2.8227
Batch 33 loss: 2.8218
Batch 34 loss: 2.8213
Batch 35 loss: 2.8204
Batch 36 loss: 2.8194
Batch 37 loss: 2.8189
Batch 38 loss: 2.8181
Batch 39 loss: 2.8177
Batch 40 loss: 2.8171
Batch 41 loss: 2.8163
Batch 42 loss: 2.8156
Batch 43 loss: 2.8152
Batch 44 loss: 2.8144
Batch 45 loss: 2.8142
Batch 46 loss: 2.8135
Batch 47 loss: 2.8129
Batch 48 loss: 2.8138
Epoch [1/100], Loss: 2.8138
Validation Loss: 2.8014
Validation Loss: 2.8016
Batch 0 loss: 2.7879
Batch 1 loss: 2.7870
Batch 2 loss: 2.7882
Batch 3 loss: 2.7909
Batch 4 loss: 2.7905
Batch 5 loss: 2.7915
Batch 6 loss: 2.7894
Batch 7 loss: 2.7866
Batch 8 loss: 2.7870
Batch 9 loss: 2.7866
Batch 10 loss: 2.7874
Batch 11 loss: 2.7866
Batch 12 loss: 2.7859
Batch 13 loss: 2.7861
Batch 14 loss: 2.7852
Batch 15 loss: 2.7856
Batch 16 loss: 2.7856
Batch 17 loss: 2.7852
Batch 18 loss: 2.7854
Batch 19 loss: 2.7857
Batch 20 loss: 2.7859
Batch 21 loss: 2.7856
Batch 22 loss: 2.7858
Batch 23 loss: 2.7860
Batch 24 loss: 2.7858
Batch 25 loss: 2.7858
Batch 26 loss: 2.7856
Batch 27 loss: 2.7856
Batch 28 loss: 2.7853
Batch 29 loss: 2.7852
Batch 30 loss: 2.7846
Batch 31 loss: 2.7850
Batch 32 loss: 2.7849
Batch 33 loss: 2.7849
Batch 34 loss: 2.7843
Batch 35 loss: 2.7839
Batch 36 loss: 2.7838
Batch 37 loss: 2.7837
Batch 38 loss: 2.7836
Batch 39 loss: 2.7838
Batch 40 loss: 2.7837
Batch 41 loss: 2.7839
Batch 42 loss: 2.7838
Batch 43 loss: 2.7836
Batch 44 loss: 2.7835
Batch 45 loss: 2.7835
Batch 46 loss: 2.7831
Batch 47 loss: 2.7829
Batch 48 loss: 2.7832
Epoch [2/100], Loss: 2.7832
Validation Loss: 2.7877
Validation Loss: 2.7869
Batch 0 loss: 2.7746
Batch 1 loss: 2.7705
Batch 2 loss: 2.7725
Batch 3 loss: 2.7734
Batch 4 loss: 2.7767
Batch 5 loss: 2.7782
Batch 6 loss: 2.7758
Batch 7 loss: 2.7761
Batch 8 loss: 2.7741
Batch 9 loss: 2.7731
Batch 10 loss: 2.7723
Batch 11 loss: 2.7714
Batch 12 loss: 2.7711
Batch 13 loss: 2.7720
Batch 14 loss: 2.7719
Batch 15 loss: 2.7724
Batch 16 loss: 2.7718
Batch 17 loss: 2.7705
Batch 18 loss: 2.7696
Batch 19 loss: 2.7689
Batch 20 loss: 2.7696
Batch 21 loss: 2.7693
Batch 22 loss: 2.7696
Batch 23 loss: 2.7702
Batch 24 loss: 2.7699
Batch 25 loss: 2.7691
Batch 26 loss: 2.7689
Batch 27 loss: 2.7681
Batch 28 loss: 2.7679
Batch 29 loss: 2.7674
Batch 30 loss: 2.7667
Batch 31 loss: 2.7669
Batch 32 loss: 2.7670
Batch 33 loss: 2.7670
Batch 34 loss: 2.7673
Batch 35 loss: 2.7676
Batch 36 loss: 2.7674
Batch 37 loss: 2.7672
Batch 38 loss: 2.7675
Batch 39 loss: 2.7674
Batch 40 loss: 2.7675
Batch 41 loss: 2.7675
Batch 42 loss: 2.7676
Batch 43 loss: 2.7676
Batch 44 loss: 2.7675
Batch 45 loss: 2.7674
Batch 46 loss: 2.7670
Batch 47 loss: 2.7666
Batch 48 loss: 2.7666
Epoch [3/100], Loss: 2.7666
Validation Loss: 2.7741
Validation Loss: 2.7755
Batch 0 loss: 2.7555
Batch 1 loss: 2.7561
Batch 2 loss: 2.7605
Batch 3 loss: 2.7588
Batch 4 loss: 2.7590
Batch 5 loss: 2.7596
Batch 6 loss: 2.7591
Batch 7 loss: 2.7613
Batch 8 loss: 2.7609
Batch 9 loss: 2.7594
Batch 10 loss: 2.7581
Batch 11 loss: 2.7590
Batch 12 loss: 2.7594
Batch 13 loss: 2.7603
Batch 14 loss: 2.7596
Batch 15 loss: 2.7599
Batch 16 loss: 2.7591
Batch 17 loss: 2.7591
Batch 18 loss: 2.7595
Batch 19 loss: 2.7591
Batch 20 loss: 2.7582
Batch 21 loss: 2.7585
Batch 22 loss: 2.7576
Batch 23 loss: 2.7575
Batch 24 loss: 2.7567
Batch 25 loss: 2.7564
Batch 26 loss: 2.7562
Batch 27 loss: 2.7559
Batch 28 loss: 2.7560
Batch 29 loss: 2.7566
Batch 30 loss: 2.7567
Batch 31 loss: 2.7568
Batch 32 loss: 2.7569
Batch 33 loss: 2.7575
Batch 34 loss: 2.7572
Batch 35 loss: 2.7570
Batch 36 loss: 2.7562
Batch 37 loss: 2.7561
Batch 38 loss: 2.7563
Batch 39 loss: 2.7558
Batch 40 loss: 2.7550
Batch 41 loss: 2.7549
Batch 42 loss: 2.7551
Batch 43 loss: 2.7546
Batch 44 loss: 2.7543
Batch 45 loss: 2.7548
Batch 46 loss: 2.7545
Batch 47 loss: 2.7538
Batch 48 loss: 2.7550
Epoch [4/100], Loss: 2.7550
Validation Loss: 2.7687
Validation Loss: 2.7675
Batch 0 loss: 2.7360
Batch 1 loss: 2.7410
Batch 2 loss: 2.7439
Batch 3 loss: 2.7392
Batch 4 loss: 2.7407
Batch 5 loss: 2.7437
Batch 6 loss: 2.7473
Batch 7 loss: 2.7477
Batch 8 loss: 2.7463
Batch 9 loss: 2.7455
Batch 10 loss: 2.7460
Batch 11 loss: 2.7463
Batch 12 loss: 2.7441
Batch 13 loss: 2.7461
Batch 14 loss: 2.7468
Batch 15 loss: 2.7460
Batch 16 loss: 2.7454
Batch 17 loss: 2.7460
Batch 18 loss: 2.7448
Batch 19 loss: 2.7445
Batch 20 loss: 2.7449
Batch 21 loss: 2.7444
Batch 22 loss: 2.7437
Batch 23 loss: 2.7436
Batch 24 loss: 2.7435
Batch 25 loss: 2.7431
Batch 26 loss: 2.7436
Batch 27 loss: 2.7430
Batch 28 loss: 2.7429
Batch 29 loss: 2.7428
Batch 30 loss: 2.7433
Batch 31 loss: 2.7437
Batch 32 loss: 2.7439
Batch 33 loss: 2.7439
Batch 34 loss: 2.7440
Batch 35 loss: 2.7438
Batch 36 loss: 2.7433
Batch 37 loss: 2.7423
Batch 38 loss: 2.7424
Batch 39 loss: 2.7427
Batch 40 loss: 2.7422
Batch 41 loss: 2.7420
Batch 42 loss: 2.7417
Batch 43 loss: 2.7414
Batch 44 loss: 2.7413
Batch 45 loss: 2.7415
Batch 46 loss: 2.7409
Batch 47 loss: 2.7407
Batch 48 loss: 2.7415
Epoch [5/100], Loss: 2.7415
Validation Loss: 2.7533
Validation Loss: 2.7542
Batch 0 loss: 2.7565
Batch 1 loss: 2.7401
Batch 2 loss: 2.7397
Batch 3 loss: 2.7362
Batch 4 loss: 2.7346
Batch 5 loss: 2.7315
Batch 6 loss: 2.7335
Batch 7 loss: 2.7333
Batch 8 loss: 2.7328
Batch 9 loss: 2.7315
Batch 10 loss: 2.7325
Batch 11 loss: 2.7314
Batch 12 loss: 2.7307
Batch 13 loss: 2.7317
Batch 14 loss: 2.7328
Batch 15 loss: 2.7318
Batch 16 loss: 2.7323
Batch 17 loss: 2.7318
Batch 18 loss: 2.7321
Batch 19 loss: 2.7314
Batch 20 loss: 2.7312
Batch 21 loss: 2.7310
Batch 22 loss: 2.7313
Batch 23 loss: 2.7310
Batch 24 loss: 2.7311
Batch 25 loss: 2.7316
Batch 26 loss: 2.7318
Batch 27 loss: 2.7325
Batch 28 loss: 2.7326
Batch 29 loss: 2.7323
Batch 30 loss: 2.7322
Batch 31 loss: 2.7325
Batch 32 loss: 2.7329
Batch 33 loss: 2.7326
Batch 34 loss: 2.7330
Batch 35 loss: 2.7332
Batch 36 loss: 2.7328
Batch 37 loss: 2.7329
Batch 38 loss: 2.7327
Batch 39 loss: 2.7324
Batch 40 loss: 2.7324
Batch 41 loss: 2.7320
Batch 42 loss: 2.7319
Batch 43 loss: 2.7312
Batch 44 loss: 2.7311
Batch 45 loss: 2.7311
Batch 46 loss: 2.7312
Batch 47 loss: 2.7306
Batch 48 loss: 2.7304
Epoch [6/100], Loss: 2.7304
Validation Loss: 2.7564
Validation Loss: 2.7572
Batch 0 loss: 2.7356
Batch 1 loss: 2.7383
Batch 2 loss: 2.7322
Batch 3 loss: 2.7340
Batch 4 loss: 2.7340
Batch 5 loss: 2.7313
Batch 6 loss: 2.7267
Batch 7 loss: 2.7252
Batch 8 loss: 2.7240
Batch 9 loss: 2.7239
Batch 10 loss: 2.7246
Batch 11 loss: 2.7240
Batch 12 loss: 2.7248
Batch 13 loss: 2.7252
Batch 14 loss: 2.7242
Batch 15 loss: 2.7243
Batch 16 loss: 2.7236
Batch 17 loss: 2.7221
Batch 18 loss: 2.7221
Batch 19 loss: 2.7219
Batch 20 loss: 2.7230
Batch 21 loss: 2.7233
Batch 22 loss: 2.7225
Batch 23 loss: 2.7220
Batch 24 loss: 2.7216
Batch 25 loss: 2.7209
Batch 26 loss: 2.7211
Batch 27 loss: 2.7215
Batch 28 loss: 2.7220
Batch 29 loss: 2.7219
Batch 30 loss: 2.7223
Batch 31 loss: 2.7221
Batch 32 loss: 2.7219
Batch 33 loss: 2.7219
Batch 34 loss: 2.7221
Batch 35 loss: 2.7223
Batch 36 loss: 2.7224
Batch 37 loss: 2.7225
Batch 38 loss: 2.7230
Batch 39 loss: 2.7228
Batch 40 loss: 2.7232
Batch 41 loss: 2.7228
Batch 42 loss: 2.7229
Batch 43 loss: 2.7226
Batch 44 loss: 2.7224
Batch 45 loss: 2.7218
Batch 46 loss: 2.7217
Batch 47 loss: 2.7218
Batch 48 loss: 2.7217
Epoch [7/100], Loss: 2.7217
Validation Loss: 2.7480
Validation Loss: 2.7483
Batch 0 loss: 2.7358
Batch 1 loss: 2.7353
Batch 2 loss: 2.7278
Batch 3 loss: 2.7237
Batch 4 loss: 2.7194
Batch 5 loss: 2.7191
Batch 6 loss: 2.7207
Batch 7 loss: 2.7221
Batch 8 loss: 2.7196
Batch 9 loss: 2.7180
Batch 10 loss: 2.7174
Batch 11 loss: 2.7157
Batch 12 loss: 2.7144
Batch 13 loss: 2.7149
Batch 14 loss: 2.7155
Batch 15 loss: 2.7152
Batch 16 loss: 2.7144
Batch 17 loss: 2.7138
Batch 18 loss: 2.7140
Batch 19 loss: 2.7142
Batch 20 loss: 2.7150
Batch 21 loss: 2.7155
Batch 22 loss: 2.7156
Batch 23 loss: 2.7154
Batch 24 loss: 2.7157
Batch 25 loss: 2.7162
Batch 26 loss: 2.7160
Batch 27 loss: 2.7159
Batch 28 loss: 2.7159
Batch 29 loss: 2.7172
Batch 30 loss: 2.7167
Batch 31 loss: 2.7165
Batch 32 loss: 2.7157
Batch 33 loss: 2.7152
Batch 34 loss: 2.7148
Batch 35 loss: 2.7148
Batch 36 loss: 2.7150
Batch 37 loss: 2.7149
Batch 38 loss: 2.7145
Batch 39 loss: 2.7142
Batch 40 loss: 2.7141
Batch 41 loss: 2.7146
Batch 42 loss: 2.7148
Batch 43 loss: 2.7149
Batch 44 loss: 2.7148
Batch 45 loss: 2.7147
Batch 46 loss: 2.7150
Batch 47 loss: 2.7153
Batch 48 loss: 2.7156
Epoch [8/100], Loss: 2.7156
Validation Loss: 2.7499
Validation Loss: 2.7505
Batch 0 loss: 2.7101
Batch 1 loss: 2.6991
Batch 2 loss: 2.6959
Batch 3 loss: 2.7017
Batch 4 loss: 2.7017
Batch 5 loss: 2.7048
Batch 6 loss: 2.7032
Batch 7 loss: 2.7055
Batch 8 loss: 2.7043
Batch 9 loss: 2.7061
Batch 10 loss: 2.7083
Batch 11 loss: 2.7073
Batch 12 loss: 2.7071
Batch 13 loss: 2.7063
Batch 14 loss: 2.7076
Batch 15 loss: 2.7061
Batch 16 loss: 2.7067
Batch 17 loss: 2.7066
Batch 18 loss: 2.7069
Batch 19 loss: 2.7060
Batch 20 loss: 2.7069
Batch 21 loss: 2.7068
Batch 22 loss: 2.7062
Batch 23 loss: 2.7059
Batch 24 loss: 2.7060
Batch 25 loss: 2.7053
Batch 26 loss: 2.7057
Batch 27 loss: 2.7049
Batch 28 loss: 2.7058
Batch 29 loss: 2.7065
Batch 30 loss: 2.7063
Batch 31 loss: 2.7059
Batch 32 loss: 2.7060
Batch 33 loss: 2.7063
Batch 34 loss: 2.7070
Batch 35 loss: 2.7074
Batch 36 loss: 2.7073
Batch 37 loss: 2.7075
Batch 38 loss: 2.7080
Batch 39 loss: 2.7077
Batch 40 loss: 2.7076
Batch 41 loss: 2.7075
Batch 42 loss: 2.7073
Batch 43 loss: 2.7071
Batch 44 loss: 2.7069
Batch 45 loss: 2.7067
Batch 46 loss: 2.7073
Batch 47 loss: 2.7078
Batch 48 loss: 2.7089
Epoch [9/100], Loss: 2.7089
Validation Loss: 2.7428
Validation Loss: 2.7425
Batch 0 loss: 2.7048
Batch 1 loss: 2.6958
Batch 2 loss: 2.7008
Batch 3 loss: 2.7016
Batch 4 loss: 2.7019
Batch 5 loss: 2.7006
Batch 6 loss: 2.7006
Batch 7 loss: 2.7006
Batch 8 loss: 2.7019
Batch 9 loss: 2.6995
Batch 10 loss: 2.6977
Batch 11 loss: 2.6962
Batch 12 loss: 2.6987
Batch 13 loss: 2.7008
Batch 14 loss: 2.7007
Batch 15 loss: 2.7000
Batch 16 loss: 2.6994
Batch 17 loss: 2.6993
Batch 18 loss: 2.7006
Batch 19 loss: 2.7017
Batch 20 loss: 2.7014
Batch 21 loss: 2.7006
Batch 22 loss: 2.7005
Batch 23 loss: 2.7010
Batch 24 loss: 2.7012
Batch 25 loss: 2.7012
Batch 26 loss: 2.7012
Batch 27 loss: 2.7012
Batch 28 loss: 2.7009
Batch 29 loss: 2.7009
Batch 30 loss: 2.7003
Batch 31 loss: 2.7002
Batch 32 loss: 2.7004
Batch 33 loss: 2.6999
Batch 34 loss: 2.6993
Batch 35 loss: 2.6997
Batch 36 loss: 2.7007
Batch 37 loss: 2.7007
Batch 38 loss: 2.7016
Batch 39 loss: 2.7015
Batch 40 loss: 2.7013
Batch 41 loss: 2.7011
Batch 42 loss: 2.7009
Batch 43 loss: 2.7011
Batch 44 loss: 2.7007
Batch 45 loss: 2.7011
Batch 46 loss: 2.7009
Batch 47 loss: 2.7006
Batch 48 loss: 2.7008
Epoch [10/100], Loss: 2.7008
Validation Loss: 2.7395
Validation Loss: 2.7402
Batch 0 loss: 2.6678
Batch 1 loss: 2.6687
Batch 2 loss: 2.6837
Batch 3 loss: 2.6856
Batch 4 loss: 2.6889
Batch 5 loss: 2.6905
Batch 6 loss: 2.6912

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23784653: <transf_vae_4> in cluster <dcc> Exited

Job <transf_vae_4> was submitted from host <hpclogin1> by user <s233201> in cluster <dcc> at Fri Jan 17 16:03:38 2025
Job was executed on host(s) <4*n-62-18-12>, in queue <c27666>, as user <s233201> in cluster <dcc> at Fri Jan 17 16:03:50 2025
</zhome/85/8/203063> was used as the home directory.
</zhome/85/8/203063/pai_course> was used as the working directory.
Started at Fri Jan 17 16:03:50 2025
Terminated at Fri Jan 17 22:52:52 2025
Results reported at Fri Jan 17 22:52:52 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
### General options
### –- specify queue --
#BSUB -q c27666 
### -- set the job Name --
#BSUB -J transf_vae_4
### -- ask for number of cores (default: 1) --
#BSUB -n 4
### -- Select the resources: 1 gpu in exclusive process mode --
#BSUB -gpu "num=1:mode=exclusive_process"
### -- set walltime limit: hh:mm --  maximum 24 hours for GPU-queues right now
#BSUB -W 8:00
# request 5GB of system-memory
#BSUB -R "rusage[mem=16GB]"
### -- set the email address --
# please uncomment the following line and put in your e-mail address,
# if you want to receive e-mail notifications on a non-default address
##BSUB -u your_email_address
### -- send notification at start --
##BSUB -B
### -- send notification at completion--
##BSUB -N
### -- Specify the output and error file. %J is the job-id --
### -- -o and -e mean append, -oo and -eo mean overwrite --
#BSUB -o project/out/gpu_%J.out
#BSUB -e project/out/gpu_%J.err
# -- end of LSF options --

nvidia-smi
# Load the cuda module
module load cuda/11.6

source pai/bin/activate

python -u project/transf_vae_v4.py 




------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24558.21 sec.
    Max Memory :                                 1171 MB
    Average Memory :                             1162.49 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               64365.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                13
    Run time :                                   24542 sec.
    Turnaround time :                            24554 sec.

The output (if any) is above this job summary.



PS:

Read file <project/out/gpu_23784653.err> for stderr output of this job.

