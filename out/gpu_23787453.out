Sat Jan 18 11:32:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:CA:00.0 Off |                   On |
| N/A   31C    P0             61W /  300W |      88MiB /  81920MiB |     N/A      Default |
|                                         |                        |              Enabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| MIG devices:                                                                            |
+------------------+----------------------------------+-----------+-----------------------+
| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|        Shared         |
|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC  DEC  OFA  JPG |
|                  |                                  |        ECC|                       |
|==================+==================================+===========+=======================|
|  0    5   0   0  |              25MiB / 19968MiB    | 28      0 |  2   0    1    0    0 |
|                  |                 0MiB / 32767MiB  |           |                       |
+------------------+----------------------------------+-----------+-----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+



['<s>', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', '<eos>']
Vocabulary: {'S': 19, 'Y': 23, '<s>': 3, 'E': 7, 'N': 15, 'H': 10, 'R': 18, 'A': 4, 'G': 9, '<eos>': 2, 'L': 13, 'D': 6, 'M': 14, 'V': 21, 'I': 11, 'W': 22, '<pad>': 0, '<mask>': 1, 'F': 8, 'Q': 17, 'P': 16, 'K': 12, 'T': 20, 'C': 5}
Vocabulary size: 24
                               ID                                           Sequence
0  tr|A0A011QCF7|A0A011QCF7_9PROT  MLPGRTPAAQLKLTILHIDARENNMDQSNRYADLSLREEDLIAGGK...
1  tr|A0A021X0E6|A0A021X0E6_9HYPH  MIRLTYRIETAGSPEAMAAKIASDQSTGTFVALPGETEELKARVAA...
2  tr|A0A023CSQ7|A0A023CSQ7_9BACI  MSQVIATYLIHDEKDIKKKAEGIALGLTVGTWTDLPLLEQEQLRKH...
3  tr|A0A023D5D1|A0A023D5D1_ACIMT  MNEITEIRGRDRYRAGVLKYAQMGYWDSDYTPSDTDLLALFRITPQ...
4  tr|A0A023PKS2|A0A023PKS2_9STRA  MFQSVEERTRIKNERYESGVIPYAEMGYWDANYTIKDTDVLALFRI...
Train dataset size: 3080
Validation dataset size: 385
Test dataset size: 385
Batch 0 loss: 3.2895
Batch 1 loss: 3.2031
Batch 2 loss: 3.1528
Batch 3 loss: 3.1155
Batch 4 loss: 3.0858
Batch 5 loss: 3.0634
Batch 6 loss: 3.0461
Batch 7 loss: 3.0316
Batch 8 loss: 3.0201
Batch 9 loss: 3.0094
Batch 10 loss: 3.0003
Batch 11 loss: 2.9931
Batch 12 loss: 2.9870
Batch 13 loss: 2.9817
Batch 14 loss: 2.9761
Batch 15 loss: 2.9716
Batch 16 loss: 2.9676
Batch 17 loss: 2.9637
Batch 18 loss: 2.9603
Batch 19 loss: 2.9571
Batch 20 loss: 2.9540
Batch 21 loss: 2.9513
Batch 22 loss: 2.9490
Batch 23 loss: 2.9468
Batch 24 loss: 2.9447
Batch 25 loss: 2.9429
Batch 26 loss: 2.9410
Batch 27 loss: 2.9394
Batch 28 loss: 2.9376
Batch 29 loss: 2.9362
Batch 30 loss: 2.9348
Batch 31 loss: 2.9332
Batch 32 loss: 2.9317
Batch 33 loss: 2.9303
Batch 34 loss: 2.9287
Batch 35 loss: 2.9276
Batch 36 loss: 2.9262
Batch 37 loss: 2.9249
Batch 38 loss: 2.9235
Batch 39 loss: 2.9224
Batch 40 loss: 2.9214
Batch 41 loss: 2.9202
Batch 42 loss: 2.9194
Batch 43 loss: 2.9183
Batch 44 loss: 2.9176
Batch 45 loss: 2.9167
Batch 46 loss: 2.9159
Batch 47 loss: 2.9149
Batch 48 loss: 2.9137
Batch 49 loss: 2.9131
Batch 50 loss: 2.9122
Batch 51 loss: 2.9114
Batch 52 loss: 2.9105
Batch 53 loss: 2.9096
Batch 54 loss: 2.9090
Batch 55 loss: 2.9083
Batch 56 loss: 2.9074
Batch 57 loss: 2.9064
Batch 58 loss: 2.9056
Batch 59 loss: 2.9050
Batch 60 loss: 2.9042
Batch 61 loss: 2.9036
Batch 62 loss: 2.9027
Batch 63 loss: 2.9017
Batch 64 loss: 2.9013
Batch 65 loss: 2.9008
Batch 66 loss: 2.9003
Batch 67 loss: 2.8998
Batch 68 loss: 2.8993
Batch 69 loss: 2.8986
Batch 70 loss: 2.8980
Batch 71 loss: 2.8976
Batch 72 loss: 2.8969
Batch 73 loss: 2.8966
Batch 74 loss: 2.8960
Batch 75 loss: 2.8954
Batch 76 loss: 2.8950
Batch 77 loss: 2.8945
Batch 78 loss: 2.8940
Batch 79 loss: 2.8935
Batch 80 loss: 2.8930
Batch 81 loss: 2.8924
Batch 82 loss: 2.8919
Batch 83 loss: 2.8914
Batch 84 loss: 2.8909
Batch 85 loss: 2.8904
Batch 86 loss: 2.8899
Batch 87 loss: 2.8894
Batch 88 loss: 2.8892
Batch 89 loss: 2.8888
Batch 90 loss: 2.8885
Batch 91 loss: 2.8880
Batch 92 loss: 2.8876
Batch 93 loss: 2.8871
Batch 94 loss: 2.8867
Batch 95 loss: 2.8863
Batch 96 loss: 2.8859
Epoch [1/100], Loss: 2.8859
Validation Loss: 2.8494
Validation Loss: 2.8494
Batch 0 loss: 2.8344
Batch 1 loss: 2.8346
Batch 2 loss: 2.8355
Batch 3 loss: 2.8356
Batch 4 loss: 2.8400
Batch 5 loss: 2.8406
Batch 6 loss: 2.8395
Batch 7 loss: 2.8377
Batch 8 loss: 2.8381
Batch 9 loss: 2.8385
Batch 10 loss: 2.8386
Batch 11 loss: 2.8393
Batch 12 loss: 2.8374
Batch 13 loss: 2.8368
Batch 14 loss: 2.8365
Batch 15 loss: 2.8356
Batch 16 loss: 2.8357
Batch 17 loss: 2.8356
Batch 18 loss: 2.8351
Batch 19 loss: 2.8348
Batch 20 loss: 2.8342
Batch 21 loss: 2.8336
Batch 22 loss: 2.8332
Batch 23 loss: 2.8334
Batch 24 loss: 2.8333
Batch 25 loss: 2.8331
Batch 26 loss: 2.8337
Batch 27 loss: 2.8333
Batch 28 loss: 2.8332
Batch 29 loss: 2.8327
Batch 30 loss: 2.8323
Batch 31 loss: 2.8323
Batch 32 loss: 2.8322
Batch 33 loss: 2.8322
Batch 34 loss: 2.8322
Batch 35 loss: 2.8321
Batch 36 loss: 2.8319
Batch 37 loss: 2.8319
Batch 38 loss: 2.8319
Batch 39 loss: 2.8319
Batch 40 loss: 2.8318
Batch 41 loss: 2.8316
Batch 42 loss: 2.8316
Batch 43 loss: 2.8312
Batch 44 loss: 2.8307
Batch 45 loss: 2.8304
Batch 46 loss: 2.8299
Batch 47 loss: 2.8299
Batch 48 loss: 2.8300
Batch 49 loss: 2.8301
Batch 50 loss: 2.8302
Batch 51 loss: 2.8303
Batch 52 loss: 2.8301
Batch 53 loss: 2.8299
Batch 54 loss: 2.8297
Batch 55 loss: 2.8298
Batch 56 loss: 2.8295
Batch 57 loss: 2.8296
Batch 58 loss: 2.8294
Batch 59 loss: 2.8293
Batch 60 loss: 2.8292
Batch 61 loss: 2.8291
Batch 62 loss: 2.8288
Batch 63 loss: 2.8287
Batch 64 loss: 2.8286
Batch 65 loss: 2.8285
Batch 66 loss: 2.8284
Batch 67 loss: 2.8281
Batch 68 loss: 2.8277
Batch 69 loss: 2.8280
Batch 70 loss: 2.8277
Batch 71 loss: 2.8275
Batch 72 loss: 2.8274
Batch 73 loss: 2.8272
Batch 74 loss: 2.8269
Batch 75 loss: 2.8268
Batch 76 loss: 2.8268
Batch 77 loss: 2.8265
Batch 78 loss: 2.8263
Batch 79 loss: 2.8261
Batch 80 loss: 2.8258
Batch 81 loss: 2.8259
Batch 82 loss: 2.8254
Batch 83 loss: 2.8252
Batch 84 loss: 2.8251
Batch 85 loss: 2.8249
Batch 86 loss: 2.8247
Batch 87 loss: 2.8245
Batch 88 loss: 2.8245
Batch 89 loss: 2.8246
Batch 90 loss: 2.8246
Batch 91 loss: 2.8244
Batch 92 loss: 2.8245
Batch 93 loss: 2.8244
Batch 94 loss: 2.8242
Batch 95 loss: 2.8239
Batch 96 loss: 2.8237
Epoch [2/100], Loss: 2.8237
Validation Loss: 2.8144
Validation Loss: 2.8148
Batch 0 loss: 2.8115
Batch 1 loss: 2.8073
Batch 2 loss: 2.8079
Batch 3 loss: 2.8070
Batch 4 loss: 2.8091
Batch 5 loss: 2.8060
Batch 6 loss: 2.8074
Batch 7 loss: 2.8071
Batch 8 loss: 2.8062
Batch 9 loss: 2.8057
Batch 10 loss: 2.8048
Batch 11 loss: 2.8037
Batch 12 loss: 2.8028
Batch 13 loss: 2.8033
Batch 14 loss: 2.8029
Batch 15 loss: 2.8030
Batch 16 loss: 2.8026
Batch 17 loss: 2.8024
Batch 18 loss: 2.8026
Batch 19 loss: 2.8030
Batch 20 loss: 2.8029
Batch 21 loss: 2.8025
Batch 22 loss: 2.8024
Batch 23 loss: 2.8021
Batch 24 loss: 2.8019
Batch 25 loss: 2.8022
Batch 26 loss: 2.8018
Batch 27 loss: 2.8025
Batch 28 loss: 2.8018
Batch 29 loss: 2.8016
Batch 30 loss: 2.8020
Batch 31 loss: 2.8011
Batch 32 loss: 2.8009
Batch 33 loss: 2.8005
Batch 34 loss: 2.8005
Batch 35 loss: 2.8004
Batch 36 loss: 2.8001
Batch 37 loss: 2.8001
Batch 38 loss: 2.7998
Batch 39 loss: 2.7992
Batch 40 loss: 2.7990
Batch 41 loss: 2.7990
Batch 42 loss: 2.7988
Batch 43 loss: 2.7986
Batch 44 loss: 2.7983
Batch 45 loss: 2.7985
Batch 46 loss: 2.7987
Batch 47 loss: 2.7984
Batch 48 loss: 2.7978
Batch 49 loss: 2.7977
Batch 50 loss: 2.7980
Batch 51 loss: 2.7976
Batch 52 loss: 2.7973
Batch 53 loss: 2.7976
Batch 54 loss: 2.7975
Batch 55 loss: 2.7973
Batch 56 loss: 2.7969
Batch 57 loss: 2.7967
Batch 58 loss: 2.7965
Batch 59 loss: 2.7965
Batch 60 loss: 2.7963
Batch 61 loss: 2.7960
Batch 62 loss: 2.7960
Batch 63 loss: 2.7960
Batch 64 loss: 2.7958
Batch 65 loss: 2.7955
Batch 66 loss: 2.7954
Batch 67 loss: 2.7956
Batch 68 loss: 2.7957
Batch 69 loss: 2.7958
Batch 70 loss: 2.7956
Batch 71 loss: 2.7954
Batch 72 loss: 2.7950
Batch 73 loss: 2.7946
Batch 74 loss: 2.7947
Batch 75 loss: 2.7944
Batch 76 loss: 2.7942
Batch 77 loss: 2.7940
Batch 78 loss: 2.7938
Batch 79 loss: 2.7937
Batch 80 loss: 2.7934
Batch 81 loss: 2.7931
Batch 82 loss: 2.7930
Batch 83 loss: 2.7928
Batch 84 loss: 2.7927
Batch 85 loss: 2.7927
Batch 86 loss: 2.7926
Batch 87 loss: 2.7924
Batch 88 loss: 2.7921
Batch 89 loss: 2.7919
Batch 90 loss: 2.7916
Batch 91 loss: 2.7914
Batch 92 loss: 2.7913
Batch 93 loss: 2.7912
Batch 94 loss: 2.7911
Batch 95 loss: 2.7909
Batch 96 loss: 2.7908
Epoch [3/100], Loss: 2.7908
Validation Loss: 2.7811
Validation Loss: 2.7823
Batch 0 loss: 2.7984
Batch 1 loss: 2.7812
Batch 2 loss: 2.7773
Batch 3 loss: 2.7771
Batch 4 loss: 2.7818
Batch 5 loss: 2.7819
Batch 6 loss: 2.7820
Batch 7 loss: 2.7821
Batch 8 loss: 2.7830
Batch 9 loss: 2.7796
Batch 10 loss: 2.7793
Batch 11 loss: 2.7802
Batch 12 loss: 2.7804
Batch 13 loss: 2.7803
Batch 14 loss: 2.7800
Batch 15 loss: 2.7816
Batch 16 loss: 2.7813
Batch 17 loss: 2.7814
Batch 18 loss: 2.7810
Batch 19 loss: 2.7799
Batch 20 loss: 2.7788
Batch 21 loss: 2.7773
Batch 22 loss: 2.7770
Batch 23 loss: 2.7774
Batch 24 loss: 2.7765
Batch 25 loss: 2.7758
Batch 26 loss: 2.7759
Batch 27 loss: 2.7759
Batch 28 loss: 2.7756
Batch 29 loss: 2.7759
Batch 30 loss: 2.7756
Batch 31 loss: 2.7757
Batch 32 loss: 2.7755
Batch 33 loss: 2.7750
Batch 34 loss: 2.7755
Batch 35 loss: 2.7750
Batch 36 loss: 2.7747
Batch 37 loss: 2.7744
Batch 38 loss: 2.7739
Batch 39 loss: 2.7734
Batch 40 loss: 2.7733
Batch 41 loss: 2.7732
Batch 42 loss: 2.7729
Batch 43 loss: 2.7725
Batch 44 loss: 2.7724
Batch 45 loss: 2.7724
Batch 46 loss: 2.7718
Batch 47 loss: 2.7714
Batch 48 loss: 2.7717
Batch 49 loss: 2.7716
Batch 50 loss: 2.7712
Batch 51 loss: 2.7713
Batch 52 loss: 2.7711
Batch 53 loss: 2.7712
Batch 54 loss: 2.7707
Batch 55 loss: 2.7702
Batch 56 loss: 2.7701
Batch 57 loss: 2.7701
Batch 58 loss: 2.7699
Batch 59 loss: 2.7694
Batch 60 loss: 2.7692
Batch 61 loss: 2.7692
Batch 62 loss: 2.7691
Batch 63 loss: 2.7689
Batch 64 loss: 2.7687
Batch 65 loss: 2.7685
Batch 66 loss: 2.7684
Batch 67 loss: 2.7685
Batch 68 loss: 2.7684
Batch 69 loss: 2.7680
Batch 70 loss: 2.7678
Batch 71 loss: 2.7673
Batch 72 loss: 2.7673
Batch 73 loss: 2.7673
Batch 74 loss: 2.7669
Batch 75 loss: 2.7668
Batch 76 loss: 2.7666
Batch 77 loss: 2.7669
Batch 78 loss: 2.7670
Batch 79 loss: 2.7670
Batch 80 loss: 2.7665
Batch 81 loss: 2.7664
Batch 82 loss: 2.7663
Batch 83 loss: 2.7659
Batch 84 loss: 2.7655
Batch 85 loss: 2.7656
Batch 86 loss: 2.7658
Batch 87 loss: 2.7656
Batch 88 loss: 2.7654
Batch 89 loss: 2.7652
Batch 90 loss: 2.7649
Batch 91 loss: 2.7646
Batch 92 loss: 2.7643
Batch 93 loss: 2.7642
Batch 94 loss: 2.7641
Batch 95 loss: 2.7639
Batch 96 loss: 2.7636
Epoch [4/100], Loss: 2.7636
Validation Loss: 2.7557
Validation Loss: 2.7556
Batch 0 loss: 2.7702
Batch 1 loss: 2.7595
Batch 2 loss: 2.7565
Batch 3 loss: 2.7603
Batch 4 loss: 2.7588
Batch 5 loss: 2.7556
Batch 6 loss: 2.7562
Batch 7 loss: 2.7555
Batch 8 loss: 2.7523
Batch 9 loss: 2.7544
Batch 10 loss: 2.7523
Batch 11 loss: 2.7532
Batch 12 loss: 2.7526
Batch 13 loss: 2.7513
Batch 14 loss: 2.7499
Batch 15 loss: 2.7481
Batch 16 loss: 2.7477
Batch 17 loss: 2.7462
Batch 18 loss: 2.7467
Batch 19 loss: 2.7468
Batch 20 loss: 2.7464
Batch 21 loss: 2.7455
Batch 22 loss: 2.7452
Batch 23 loss: 2.7458
Batch 24 loss: 2.7465
Batch 25 loss: 2.7470
Batch 26 loss: 2.7461
Batch 27 loss: 2.7465
Batch 28 loss: 2.7463
Batch 29 loss: 2.7458
Batch 30 loss: 2.7461
Batch 31 loss: 2.7455
Batch 32 loss: 2.7458
Batch 33 loss: 2.7454
Batch 34 loss: 2.7453
Batch 35 loss: 2.7447
Batch 36 loss: 2.7447
Batch 37 loss: 2.7447
Batch 38 loss: 2.7450
Batch 39 loss: 2.7452
Batch 40 loss: 2.7454
Batch 41 loss: 2.7463
Batch 42 loss: 2.7468
Batch 43 loss: 2.7463
Batch 44 loss: 2.7462
Batch 45 loss: 2.7470
Batch 46 loss: 2.7465
Batch 47 loss: 2.7466
Batch 48 loss: 2.7464
Batch 49 loss: 2.7463
Batch 50 loss: 2.7462
Batch 51 loss: 2.7455
Batch 52 loss: 2.7457
Batch 53 loss: 2.7454
Batch 54 loss: 2.7450
Batch 55 loss: 2.7449
Batch 56 loss: 2.7454
Batch 57 loss: 2.7456
Batch 58 loss: 2.7453
Batch 59 loss: 2.7452
Batch 60 loss: 2.7449
Batch 61 loss: 2.7454
Batch 62 loss: 2.7457
Batch 63 loss: 2.7454
Batch 64 loss: 2.7450
Batch 65 loss: 2.7450
Batch 66 loss: 2.7447
Batch 67 loss: 2.7445
Batch 68 loss: 2.7443
Batch 69 loss: 2.7443
Batch 70 loss: 2.7439
Batch 71 loss: 2.7441
Batch 72 loss: 2.7441
Batch 73 loss: 2.7436
Batch 74 loss: 2.7435
Batch 75 loss: 2.7434
Batch 76 loss: 2.7433
Batch 77 loss: 2.7436
Batch 78 loss: 2.7432
Batch 79 loss: 2.7429
Batch 80 loss: 2.7429
Batch 81 loss: 2.7427
Batch 82 loss: 2.7425
Batch 83 loss: 2.7425
Batch 84 loss: 2.7422
Batch 85 loss: 2.7424
Batch 86 loss: 2.7423
Batch 87 loss: 2.7422
Batch 88 loss: 2.7418
Batch 89 loss: 2.7416
Batch 90 loss: 2.7416
Batch 91 loss: 2.7413
Batch 92 loss: 2.7408
Batch 93 loss: 2.7406
Batch 94 loss: 2.7404
Batch 95 loss: 2.7405
Batch 96 loss: 2.7401
Epoch [5/100], Loss: 2.7401
Validation Loss: 2.7377
Validation Loss: 2.7366
Batch 0 loss: 2.7148
Batch 1 loss: 2.7188
Batch 2 loss: 2.7265
Batch 3 loss: 2.7268
Batch 4 loss: 2.7251
Batch 5 loss: 2.7277
Batch 6 loss: 2.7238
Batch 7 loss: 2.7218
Batch 8 loss: 2.7230
Batch 9 loss: 2.7202
Batch 10 loss: 2.7216
Batch 11 loss: 2.7234
Batch 12 loss: 2.7227
Batch 13 loss: 2.7241
Batch 14 loss: 2.7249
Batch 15 loss: 2.7285
Batch 16 loss: 2.7288
Batch 17 loss: 2.7281
Batch 18 loss: 2.7277
Batch 19 loss: 2.7262
Batch 20 loss: 2.7254
Batch 21 loss: 2.7261
Batch 22 loss: 2.7257
Batch 23 loss: 2.7253
Batch 24 loss: 2.7248
Batch 25 loss: 2.7250
Batch 26 loss: 2.7249
Batch 27 loss: 2.7251
Batch 28 loss: 2.7262
Batch 29 loss: 2.7252
Batch 30 loss: 2.7253
Batch 31 loss: 2.7251

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23787453: <transf_vae_4> in cluster <dcc> Exited

Job <transf_vae_4> was submitted from host <hpclogin1> by user <s233201> in cluster <dcc> at Sat Jan 18 11:32:29 2025
Job was executed on host(s) <4*n-62-18-12>, in queue <c27666>, as user <s233201> in cluster <dcc> at Sat Jan 18 11:32:29 2025
</zhome/85/8/203063> was used as the home directory.
</zhome/85/8/203063/pai_course> was used as the working directory.
Started at Sat Jan 18 11:32:29 2025
Terminated at Sat Jan 18 16:15:11 2025
Results reported at Sat Jan 18 16:15:11 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
### General options
### â€“- specify queue --
#BSUB -q c27666 
### -- set the job Name --
#BSUB -J transf_vae_4
### -- ask for number of cores (default: 1) --
#BSUB -n 4
### -- Select the resources: 1 gpu in exclusive process mode --
#BSUB -gpu "num=1:mode=exclusive_process"
### -- set walltime limit: hh:mm --  maximum 24 hours for GPU-queues right now
#BSUB -W 8:00
# request 5GB of system-memory
#BSUB -R "rusage[mem=16GB]"
### -- set the email address --
# please uncomment the following line and put in your e-mail address,
# if you want to receive e-mail notifications on a non-default address
##BSUB -u your_email_address
### -- send notification at start --
##BSUB -B
### -- send notification at completion--
##BSUB -N
### -- Specify the output and error file. %J is the job-id --
### -- -o and -e mean append, -oo and -eo mean overwrite --
#BSUB -o project/out/gpu_%J.out
#BSUB -e project/out/gpu_%J.err
# -- end of LSF options --

nvidia-smi
# Load the cuda module
module load cuda/11.6

source pai/bin/activate

python -u project/transf_vae_v4.py 




------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   16910.12 sec.
    Max Memory :                                 1170 MB
    Average Memory :                             1138.79 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               64366.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                13
    Run time :                                   17060 sec.
    Turnaround time :                            16962 sec.

The output (if any) is above this job summary.



PS:

Read file <project/out/gpu_23787453.err> for stderr output of this job.

